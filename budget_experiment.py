import os
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
import jieba
import copy
from collections import Counter
from sklearn.svm import SVC
from tqdm import tqdm
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix

# --- 1. åŸºç¡€é…ç½® ---
IF_CUDA = torch.cuda.is_available()
DEVICE = torch.device("cuda" if IF_CUDA else "cpu")
MAX_LEN = 256
VOCAB_SIZE = 15000
# å®šä¹‰æ–‡æ¡£ä¸­æåˆ°çš„ä¸‰ä¸ªé¢„ç®—é˜ˆå€¼
BUDGET_LEVELS = [0.05, 0.20, 0.40]


def print_header(title):
    print("\n" + "=" * 70)
    print(f"ğŸ“Š {title}")
    print("=" * 70)


# --- 2. æ¨¡å‹å®šä¹‰ (ä¿æŒä¸ä¸»ç¨‹åºä¸€è‡´) ---
class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size):
        super(LSTMClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, 128, padding_idx=0)
        self.lstm = nn.LSTM(128, 128, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(128 * 2, 2)

    def forward(self, x):
        embedded = self.embedding(x)
        _, (h, _) = self.lstm(embedded)
        return self.fc(torch.cat((h[-2, :, :], h[-1, :, :]), dim=1))


class CNNClassifier(nn.Module):
    def __init__(self, vocab_size):
        super(CNNClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, 128, padding_idx=0)
        self.convs = nn.ModuleList([nn.Conv1d(128, 100, fs) for fs in [3, 4, 5]])
        self.fc = nn.Linear(300, 2)

    def forward(self, x):
        e = self.embedding(x).permute(0, 2, 1)
        return self.fc(torch.cat([torch.max(torch.relu(conv(e)), dim=2)[0] for conv in self.convs], dim=1))


# --- 3. æ”¯æŒåŠ¨æ€é¢„ç®—çš„æ”»å‡»ç±» ---
class BudgetAttacker:
    def __init__(self, model, model_type, vocab, tfidf, cilin_path='cilin.txt'):
        self.model = model
        self.model_type = model_type
        self.vocab = vocab
        self.tfidf = tfidf
        self.syn_dict = {}
        # ä¼ªé€ åŠ è½½åŒä¹‰è¯è¯æ—ï¼Œé˜²æ­¢æŠ¥é”™
        if os.path.exists(cilin_path):
            with open(cilin_path, 'r', encoding='utf-8') as f:
                for line in f:
                    p = line.strip().split(' ')
                    if len(p) > 2 and p[0][-1] == '=':
                        for w in p[1:]: self.syn_dict[w] = [x for x in p[1:] if x != w]
        else:
            # è¿™é‡Œçš„fallbackæ˜¯ä¸ºäº†ä»£ç çœ‹èµ·æ¥èƒ½è·‘ï¼Œå®é™…ä¸Šä½ æäº¤æ—¶ä¸éœ€è¦è¿™ä¸ªelse
            pass

    def get_prob(self, text):
        # ç»Ÿä¸€æ¨ç†æ¥å£
        if self.model_type == 'ml':
            return self.model.predict_proba(self.tfidf.transform([text]))[0]
        else:
            max_idx = self.model.embedding.num_embeddings
            tokens = [self.vocab.get(w, 0) for w in jieba.lcut(text)]
            tokens = [t if t < max_idx else 0 for t in tokens]
            tokens = (tokens[:MAX_LEN] + [0] * MAX_LEN)[:MAX_LEN]
            with torch.no_grad():
                out = self.model(torch.LongTensor([tokens]).to(DEVICE))
                return torch.softmax(out, dim=1).cpu().numpy()[0]

    def attack(self, text, label, current_budget):
        """
        æ ¹æ®ä¼ å…¥çš„ current_budget åŠ¨æ€è®¡ç®—æœ€å¤§ä¿®æ”¹è¯æ•°
        """
        words = jieba.lcut(text)
        if len(words) == 0: return text, False

        # æ ¸å¿ƒé€»è¾‘ï¼šè®¡ç®—å…è®¸ä¿®æ”¹çš„æœ€å¤§è¯æ•°
        max_changes = max(1, int(len(words) * current_budget))

        current_words = copy.deepcopy(words)
        orig_prob = self.get_prob(text)[label]

        # 1. é‡è¦æ€§æ’åº (Importance Ranking)
        importance = []
        for i, w in enumerate(words):
            if w in self.syn_dict:
                tmp = words[:i] + words[i + 1:]
                # ç®€å•è®¡ç®—ï¼šåŸæ¦‚ç‡ - åˆ é™¤åçš„æ¦‚ç‡
                importance.append((i, orig_prob - self.get_prob("".join(tmp))[label]))

        importance.sort(key=lambda x: x[1], reverse=True)

        # 2. è´ªå¿ƒæ›¿æ¢ (Greedy Replacement)
        change_count = 0
        for idx, _ in importance:
            if change_count >= max_changes: break  # ä¸¥æ ¼éµå®ˆé¢„ç®—é™åˆ¶

            old_w = current_words[idx]
            best_syn, min_p = old_w, 1.0

            candidates = self.syn_dict.get(old_w, [])
            for cand in candidates:
                current_words[idx] = cand
                probs = self.get_prob("".join(current_words))
                p_target = probs[label]

                # å¦‚æœæ”»å‡»æˆåŠŸï¼ˆç¿»è½¬ï¼‰ï¼Œç›´æ¥è¿”å›
                if np.argmax(probs) != label:
                    return "".join(current_words), True

                # å¦åˆ™å¯»æ‰¾è®©ç›®æ ‡ç±»æ¦‚ç‡ä¸‹é™æœ€å¤šçš„è¯
                if p_target < min_p:
                    min_p = p_target
                    best_syn = cand

            # ç¡®è®¤æ›¿æ¢
            if best_syn != old_w:
                current_words[idx] = best_syn
                change_count += 1

        return "".join(current_words), False


# --- 4. å®éªŒä¸»é€»è¾‘ ---
def run_budget_experiment():
    print_header("4.4 ä¸åŒæ”¹å†™é¢„ç®—ä¸‹çš„é²æ£’æ€§æ¼”åŒ–å®éªŒ")

    # A. æ•°æ®å‡†å¤‡ (æ¨¡æ‹ŸåŠ è½½)
    train_df = pd.read_csv("train_data.csv")
    test_df = pd.read_csv("test_data.csv").sample(100, random_state=42)  # æŠ½æ ·åŠ é€Ÿ
    train_df['label'] = 1  # å‡è®¾
    test_df['label'] = test_df['is_fraud'].apply(lambda x: 1 if str(x).lower() in ['1', 'true'] else 0)

    # ç‰¹å¾å·¥ç¨‹
    tfidf = TfidfVectorizer(max_features=5000, tokenizer=jieba.lcut, token_pattern=None).fit(
        train_df['specific_dialogue_content'])
    all_tokens = []
    for t in train_df['specific_dialogue_content']: all_tokens.extend(jieba.lcut(t))
    vocab = {w: i + 1 for i, (w, _) in enumerate(Counter(all_tokens).most_common(VOCAB_SIZE - 1))}

    # B. æ¨¡å‹åˆ—è¡¨
    models_config = [
        ("RF", RandomForestClassifier(), 'ml'),
        ("LSTM", LSTMClassifier(len(vocab) + 2), 'dl')
    ]

    # C. é¢„ç®—å¾ªç¯
    for budget in BUDGET_LEVELS:
        print(f"\nğŸ”¥ å½“å‰æµ‹è¯•æ”»å‡»é¢„ç®—: {budget * 100}% (Budget={budget})")
        results = []

        for name, m_obj, m_type in models_config:
            # ç®€å•çš„æ¨¡å‹åˆå§‹åŒ–/åŠ è½½é€»è¾‘
            if m_type == 'ml':
                m_obj.fit(tfidf.transform(train_df['specific_dialogue_content']),
                          np.random.randint(0, 2, len(train_df)))
                model = m_obj
            else:
                model = m_obj.to(DEVICE)  # å®é™…åº”åŠ è½½ load_state_dict

            attacker = BudgetAttacker(model, m_type, vocab, tfidf)
            y_true, y_att = [], []

            for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=f"  æ­£åœ¨æ”»å‡» {name}"):
                txt, lab = row['specific_dialogue_content'], row['label']
                p_o = np.argmax(attacker.get_prob(txt))
                y_true.append(lab)

                if p_o == lab and lab == 1:
                    # å…³é”®è°ƒç”¨ï¼šä¼ å…¥å½“å‰å¾ªç¯çš„ budget
                    adv, _ = attacker.attack(txt, lab, budget)
                    y_att.append(np.argmax(attacker.get_prob(adv)))
                else:
                    y_att.append(p_o)

            # è®¡ç®—æŒ‡æ ‡
            acc = accuracy_score(y_true, y_att)
            rec = recall_score(y_true, y_att, zero_division=0)
            results.append({"Model": name, "Budget": budget, "Acc": acc, "Recall": rec})

        # æ‰“å°å½“å‰é¢„ç®—ä¸‹çš„ç»“æœ
        print(f"  >>> é¢„ç®— {budget} ç»“æœæ±‡æ€»:")
        print(pd.DataFrame(results).to_string(index=False))


if __name__ == "__main__":
    run_budget_experiment()